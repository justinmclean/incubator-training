////
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

////

include::_settings.adoc[]
:presenter_name: Your Name
:presenter_company: Your Company
:description: Introduction to Apache Airflow
:keywords: Apache, Airflow
:author: Your Name
:email: Your email
:position: Your role

== Apache Airflow

image::AirflowLogo.png[]

[.notes]
--
Introduce yourself, and your role in the Apache Airflow community.
--

== Table of Contents

* What is Apache Airflow?
* Apache Airflow - Core Concepts
* Airflow Principles
  * Airflow Principle: Scalable
    * Scaling Strategies
  * Airflow Principle: Dynamic 
  * Airflow Principle: Extensible
  * Airflow Principle: Elegant
* Airflow Integrations
* Airflow Providers packages
* Airflow Docker stack

== What is Apache Airflow?

Airflow is a platform to programmatically author, schedule and monitor workflows.
Developed initially by Airbnb in 2014 and later donated to the Apache Software Foundation, Airflow has become the de facto standard for workflow orchestration in the data engineering ecosystem

== Apache Airflow - Core Concepts

a. Directed Acyclic Graphs (DAGs)

At the heart of Airflow are Directed Acyclic Graphs (DAGs), which represent workflows as collections of tasks organized with dependencies and relationships. DAGs ensure tasks are executed in a specific order without looping back, providing a clear and logical representation of complex workflows.

b. Tasks 

Tasks are the smallest unit of work in Airflow. They are the individual units of work that make up a DAG. These are typically implemented using Operators.
Operators are the classes that define the logic of a task.

c. Operators

Operators are the classes that define the logic of a task. They are the smallest unit of work in Airflow.
Example of pre-built operators include PythonOperators, BashOperators, etc.


== Airflow Principles

Airflow is built on following principles:

* Scalable
* Dynamic
* Extensible
* Elegant

[.notes]
--
Each principle has a slide following that explains it, so you don't need
to explain this here.
--

== Airflow Principle: Scalable

- Airflow has a modular architecture, allowing for flexible deployment and scaling of individual components.
Core components include the scheduler, workers, metadata database, and web server 

What enables scaling of Airflow :

- Utilizes a message queue (e.g., Redis, RabbitMQ) to coordinate task execution across workers. This enables asynchronous communication between the scheduler and workers

- Supports multiple executor types (e.g., Celery, Kubernetes) for distributed task execution.
This allows Horizontal Scaling

== Scaling Strategies

a. Horizontal Scaling

  Add more worker nodes to increase task processing capacity.
	â€¢	Celery Executor: Scale by increasing the number of Celery workers.
	â€¢	Kubernetes Executor: Dynamically scale worker pods based on workload

b. Vertical Scaling 

Airflow Vertical Scaling

Increase CPU and memory for critical components like the Scheduler (to handle more DAGs efficiently) and Workers (to process more tasks in parallel). 
Adjust resource limits in Kubernetes, Celery, or LocalExecutor setups for optimal performance. ðŸš€

== Airflow Principle: Dynamic

Airflow pipelines are written in Python, enabling dynamic DAG generation. 
This allows workflows to be parameterized, templated, and instantiated programmatically, adapting to changing data and business logic. ðŸš€

== Airflow Principle: Extensible

Airflow allows you to create custom operators, hooks, and plugins, extending its functionality to match your specific needs. 
This helps integrate with any system, define new abstractions, and tailor workflows to your environment seamlessly. ðŸš€


== Airflow Principle: Elegant

Airflow pipelines are lean and explicit. Parametrization is built into its core using the powerful Jinja templating engine.
This allows for dynamic, reusable, and efficient DAGs, reducing redundancy and enhancing flexibility. 

== Airflow Features

Apache Airflow provides following features:

* Pure Python
* Useful UI
* Robust Integrations
* Easy to Use
* Open Source

== Airflow Feature: Pure Python

No more command-line or XML black-magic! Use standard Python features to create your workflows, including date time formats for scheduling and loops to dynamically generate tasks. This allows you to maintain full flexibility when building your workflows.

== Airflow Feature: Useful UI

Monitor, schedule and manage your workflows via a robust and modern web application. No need to learn old, cron-like interfaces. You always have full insight into the status and logs of completed and ongoing tasks.

== Airflow Feature: Robust Integrations

Airflow provides many plug-and-play operators that are ready to execute your tasks on Google Cloud Platform, Amazon Web Services, Microsoft Azure and many other third-party services. This makes Airflow easy to apply to current infrastructure and extend to next-gen technologies.

== Airflow Feature: Easy to Use

Anyone with Python knowledge can deploy a workflow. Apache Airflow does not limit the scope of your pipelines; you can use it to build ML models, transfer data, manage your infrastructure, and more.

== Airflow Feature: Open Source

Wherever you want to share your improvement you can do this by opening a PR. Itâ€™s simple as that, no barriers, no prolonged procedures. Airflow has many active users who willingly share their experiences. Have any questions? Check out our buzzing slack.

[.notes]
--
Where is the Slack? How does one join?
--

== Airflow Integrations

Airflow supports following main integrations:

* Apache Sqoop
* Google Cloud Pub/Sub
* Amazon CloudWatch Logs
* Google Kubernetes Engine
* Google Machine learning
* Amazon Athena

[.notes]
--
Ensure that this list reflects the latest state of the art.
--

== Airflow Providers packages

Airflow has providers packages include integrations with third party integrations. They are updated independently of the Apache Airflow core. Some of these are:

* Airbyte
* Amazon
* Apache Beam
* Apache Cassandra
* JIRA

[.notes]
--
This section should be periodically updated to reflect the current state
of the project.
--

== Airflow Docker stack

Airflow has an official Dockerfile and Docker image published in DockerHub as a convenience package for installation. You can extend and customize the image according to your requirements and use it in your own deployments.

== Further Sources
Refer official documents on Apache Airflow here:

* *Airflow Documentation*: https://airflow.apache.org/docs/
* *Airflow Usecases*: https://airflow.apache.org/use-cases/

